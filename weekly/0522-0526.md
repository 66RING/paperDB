
# idea

- **混沌交给编译器自动生成, 但面向人的接口仍是分层抽象的**, 从而缓解分层带来的性能问题
- 有没有可能brain每个神经元原本是一样的, 只是收到不懂训练才分化出不同功能。不然怎么解释其强大的适应力
    * 所以需要一个归一化的过程， 这样大家看到的东西都是同类了
    * x
- 算一点传一点, 边算边传


# 2023-05-26

## MSCCL trace

> msccl可以自定义算法

直接用`LD_PRELOAD`了

效果不好, 仅all2all可以考虑


# 2023-05-25

## build coconet

- MegatronLM-Model-Parallel上效果惊人
- MegatronLM-Pipeline-Parallel上效果不好(也没找到哪里做了修改)
- Nvidia-Bert在2卡V100上直接OOM


### build apex

- build from source, use the `22.04-dev` branch
- `pip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./`


# 2023-05-24

## double binary tree

- ring算法的缺点: 频繁"启动"的启动开销. The downside of rings is that latency scales linearly with the number of GPUs

## msccl source code

- 类型: `topo.cc:822`, 根据`coll`属性的值判断算法类型

```c
  const char* collectiveType;
  NCCLCHECK(xmlGetAttrStr(topNode, "coll", &collectiveType));
  int inputNChunksMultiplier = 1;
  int outputNChunksMultiplier = 1;
  if (strcmp(collectiveType, "allreduce") == 0){
    mscclAlgo->collectiveType = ncclFuncAllReduce;
  } else if (strcmp(collectiveType, "allgather") == 0){
    mscclAlgo->collectiveType = ncclFuncAllGather;
    inputNChunksMultiplier = nRanks;
  } else if (strcmp(collectiveType, "reduce") == 0){
    mscclAlgo->collectiveType = ncclFuncReduce;
  } else if (strcmp(collectiveType, "broadcast") == 0){
    mscclAlgo->collectiveType = ncclFuncBroadcast;
  } else if (strcmp(collectiveType, "alltoall") == 0){
    mscclAlgo->collectiveType = ncclFuncAllToAll;
  } else if (strcmp(collectiveType, "reduce_scatter") == 0){
    mscclAlgo->collectiveType = ncclFuncReduceScatter;
    outputNChunksMultiplier = nRanks;
  } else if (strcmp(collectiveType, "custom") == 0){
    mscclAlgo->collectiveType = ncclFuncCustomCollective;
  } else {
    WARN("MSCCL: collective type %s is not supported.", collectiveType);
    return ncclInvalidUsage;
  }
```

- 合成: TODO


# 2023-05-23



## pytorch-msccl intergration

1. build fail
    - ‘ncclAllToAll’ was not declared in this scope  =>  Submodule没有初始化完成[x] 
    - TODO: 不知道什么原因, 就先抛弃alltoall了


## learn about mscclang



# 2023-05-22

## msccl build

1. 使用MSCCL-tools生成xml(即算法编译的结果)
2. 通过环境变量指定，以便msccl运行时加载算法
    - `MSCCL_XML_FILES=test.xm`
3. 直接替换原本nccl库即可使用?
    - `LD_LIBRARY_PATH=msccl/build/lib/:$LD_LIBRARY_PATH`


## Breaking the Computation and Communication Abstraction Barrier in Distributed Machine LearningWorkloads

> 分层直接/间接导致性能问题, 但想要修改和优化就得知道底层细节和修改底层。
>
> CoCoNet, a language to describe distributed machine learning workloads and optimize them across computation and communication boundary.

- **通信的时候做计算: 流水线, 每次传输一小个chunk就可以计算了, 如此往复**
- 提供一种IR来编写计算和通信, 通过编译器自动生成目标代码

- 怎么做到打破抽象的? 根据什么原理融合?
    * 一点传一点算

TODO:


## sccl(Synthesizing Optimal Collective Algorithms)

> scclang自动生成通信算法代码

- bw optimize: 降链路拥堵
    * ring algo
- latency optimize: 增加并行
    * Pareto-optimal with respect to the class of k-synchronous algorithms
- push比pull要快10%
- 大input size的时候还是nccl好，但是可以定制一个专门处理大input size的sccl??

- TODO
    * 大小无感知, 无法重排?

- non-combining cc and combining cc, 即allreduce等需要做加法(compute)来做合并

TODO


