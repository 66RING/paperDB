# idea

- 状态机子图替换

- tile 无处不在!!!!!!!!!!
    * 因为untile是方便程序员的, 但是什么叫编译器的黄金年代, 直接tile展开就好了
- graph build system
    * make like, target graph cache
    * distributed compile, GPU powered compile!!

- 集合通信和db很像呢
    * **计划图(执行计划), 优化器, 执行器**

- **unity的模式的启发**
    * 有限的规则展开无限的可能!!!!!!
    * graph base && rule driven && state machine => chaos
    * graph base自编译操作系统

- 可否用一种SQL like 语言来定义模型??

- 缩小任务? 从整个框架的编译缩小到集合通信的编译
    * 但是如果要overlap的话可以不修改集合通信之外的东西吗?

- overlap
    * DL的iter是确定的, 可以编译塞入任务

- 编译后的图是动态图如何

- 任意两个节点里面都可以塞一个pipeline节点, 从而可以split

- 必要的组件留好接口
    * e.g. ZeRO++, 传输前, 接收后的编码

- ZeRO和NUMA aware好像挺像的


分析Unity中自动并行的实现和单机计算图转换成分布式计算图的方法和代码实现
分析TASO对代数算子的自动展开和形式化验证的代码实现
分析taccl源代码中执行器部分的实现, 使用的是msccl解释执行, 本质是集合通信算法由若干简单的p2p算子组成
调研ZeRO++对通信计算联合优化的实现


## 2023-06-30

### ZeRO++: Extremely Efficient Collective Communication for Giant Model Training

主要针对两个痛点: i) clusters with low-bandwidth, and ii) at very small batch sizes per GPU

abs: 编码压缩, 局部一致(分层replica), all2all优化ring

- 三项优化技术, 分别优化fwd, bwd, 更新梯度(reduce-scatter)
    * QuantizedWeight Communication for ZeRO (qwZ)
        + To reduce communication volume of model weights in forward propagation, we adopt block-based quantization and data pre-fetching.
    * Quantized Gradient Communication for ZeRO (qgZ)
        + To remove crossnode communication of weights during backward pass, we hold secondary model partition on each node to trade memory for communication.
    * Communication Volume Reduction
        + To minimize gradient communication during backward propagation, we design and implement a novel all-to-all based gradient quantization and reduction scheme.

- QuantizedWeight Communication for ZeRO (qwZ): 降低参数传输量
    * **参数从fp16到fp8**, 减少传输
    * 并使用**block-based quantization [8]确保精度**, 即不同的参数部分的量化方法不同以保证精度
    * we implement highly optimized quantization CUDA kernels from scratch
- Hierarchical Weight Partition for ZeRO (hpZ): 降低权重all-gather开销
    * **每个机器都有完整的模型(e.g. 存fp16的权重), 从而变成intra的all-gather**
    * 即制造一定secondary replica来提供局部性(intra): This secondary copy of FP16 parameters is replicated across multiple secondary partitions
    * e.g. 模型平均切到8x64个GPU, 但是intra的8GPU内有各自的完整模型
    * 一个tradeoff
- **Quantized Gradient Communication for ZeRO (qgZ)**
    * 主要问题是精度在这就很重要了, 因为会累积放大, 简单的转换类型不可行了
    * 比较综合的解法: 编码压缩, 分层聚合, tensor ordering
        + 用all2all实现reduce-scatter
            + 不用ring是因为ring要多次收发, 导致精度会不断被压缩
        + fp16编码成int4, 做quantized gradient的操作
            + 因为Figure 6的原因导致所需通信量变大了
        + 分层聚合(Hierarchical), Figure 7
        + 先intra node reduce再inter node并且将intra和inter的操作流水线化
            + pipelining intra-node and inter-node communication and conducting CUDA kernel fusion
        + **tensor ordering**, Figure 8: step 2, 细
            + 就是说intra + inter后结果的g的顺序不对, 所以调整一下输入的顺序
    * TODO: review
        + fp16编码成int4?

TODO: S3.3

#### QA

##### **为什么要all2all的设计集合通信算法**

因为有编码解码操作的存在, 发出编码, 接收解码。ring 算法存在多次发射的问题(也是它延迟较高的原因)。频繁的首发就导致了精度被不断压缩。

e.g. 如Figure 5 左

所以all2all在这种情况下比ring要好? 即便考虑bw的影响, 即latency的影响大于bw的影响?

那为什么不encode 整体cc decode? 比如整体encode后在sum时才decode


##### 如何用all2all实现ring算法

all2all本来就是等价的, 只不过bandwidth不太行而已


##### blocked based quantization到底是什么

> qwz uses blocked based ...

##### quantization的开销, S4

##### 原始ZeRO不是每个机器都有完整模型的么

> first, all model states are partitioned globally across all devices as in ZeRO-3

所以, 答案是yes


##### 如何实现intra, inter流水线的

##### 什么叫tensor slice reordering for correct gradient placement.


### Unity source code

> 怎么做状态机执行?如流水并行时有send就一定有recv把, 怎么知道谁做什么?AKA最后图是怎么使用的?

`generic_sequence_optimize`后图存到到了什么地方备用?

```cpp
void FFModel::compile(LossType loss_type,
                      std::vector<MetricsType> const &metrics,
                      CompMode comp_mode) {
    FFModel *model = this;
    TaskLauncher launcher(GRAPH_OPTIMIZE_TASK_ID,
                          TaskArgument(&model, sizeof(FFModel *)));
    Future future = runtime->execute_task(ctx, launcher);
    PCG::GraphOptimalViewSerialized ret = future.get_result<PCG::GraphOptimalViewSerialized>();

    Deserializer dez(ret.data, ret.total_bytes);
    // Reconstruct operators
    PCG::Graph *best_graph = new PCG::Graph(this);
    std::unordered_map<PCG::Node, MachineView> optimal_views;
    deserialize_graph_optimal_view(dez, best_graph, optimal_views);
    operators.clear();
    convert_graph_to_operators(best_graph, optimal_views);
}
```

TODO

> 执行器的逻辑?

fit()/eval()就是直接的调fwd, bwd...等。又调到了底层c(flexflow)的fwd,bwd

```cpp
void FFModel::forward(int seq_length) {
  iter_config.seq_length = seq_length;
  for (size_t i = 0; i < operators.size(); i++) {
    operators[i]->forward(*this);
  }
}
```

怎么体现流水并行, 数据并行


TODO: 再看看`apply_fusion()`

那compile好后数据是保存到哪里了?

> MachineView怎么生成的?

TODO

> 怎么做split, 怎么体现可扩展性

TODO


> 说P变A变顺序是有要求的，怎么解决呢?

TODO


> 反向传播是否和fwd差不多?

TODO


## 2023-06-29

### Unity source code

```
register_flexflow_internal_tasks 

FFModel::compile -> TaskLauncher launcher(GRAPH_OPTIMIZE_TASK_ID)
-> graph_optimize_task -> try_one_lambda -> graph_optimize 
-> GraphSearchHelper::graph_optimize -> generic_sequence_optimize
-> base_optimize(metaflow的pop和run) -> execute_sequence_split
```


> 怎么做machine map, 做machine map来干嘛? 然后send, recv这种对偶的关系是怎么执行的呢?(内部有个状态机?)

A machine mapping is a map from task indices (i1, ... , in)
to individual GPUs that will be used to run that parallel task.

(S3 & Figure 7) & (5.2 Finding Optimized Machine Mappings Figure 13)

TODO: 搞清楚最后计算图是怎么在分布式环境下跑起来的。

先从单算子入手考虑, e.g.

```
partition partition
|   |
v   V
MatMul
  |
  V
reduce
```

S5.2:

1. 找瓶颈节点:
    1. `bottleneck = find_split_node()`, 如S5.2找post dominator node这些东西
    2. `std::tie(pre_graph, post_graph) = split_at_node(bottleneck)`
    3. `GraphSearchHelper::execute_sequence_split(pre_graph, post_graph)`递归

partitions an input PCG G by finding
a postdominator node n, such that **all paths from the inputs
to the outputs of G go through n.**

2. sequence graph split: split瓶颈节点成前后两部分, 降低全局最优成两个局部最优
    - 流水并行
    - `substitution.cc::2691`
    - cost = pre graph + post graph
    - `GraphSearchHelper::execute_sequence_split(pre_graph, post_graph)`

3. parallel graph split: 拆成两独立的子图, 尝试串行跑和并行跑, 选最优
    - 做了Partition后就会自动出现这样的子图
    - 然后两边的子图独立做优化, 然后可以使用缓存来快速同步两边子图

那`pre_graph`, `post_graph`分出来后存到了哪里?

所以这个只是做并行, 而且graph就是状态机? 然后真正的执行需要用到S3中描述的machine mapping吗?

S3.2:

PS: 使用了Legion这个cpp异步库

在`convert_graph_to_operators(best_graph, optimal_views)`中把MachineView信息赋值给节点

在`set_argumentmap_for_init`中会把view绑定到task的参数中: handle ??

需要查看执行器的执行逻辑。

TODO, 示例代码


> 有没有张量并行??

回想一下megatron是怎么做的: 数据并行 = "分布式文件系统", 流水并行 = p2p状态机, 张量并行 = 算子拆分

所以这里的张量并行是partition + reduce


## 2023-06-28

### Unity source code

> PCG中的P操作怎么结合进子图替换里的??

```
register_flexflow_internal_tasks 

FFModel::compile -> TaskLauncher launcher(GRAPH_OPTIMIZE_TASK_ID)
-> graph_optimize_task -> try_one_lambda -> graph_optimize 
-> GraphSearchHelper::graph_optimize -> generic_sequence_optimize
-> base_optimize(metaflow的pop和run) -> execute_sequence_split
```

感谢这个[talk](https://youtu.be/xRNo5M--2Go?t=620)，本质就是从单个算子出发，将算子变复杂后(分布式版)再用复杂的版本去进行子图替换以探索更多可能。

也是子图替换(以某个算子为切入点), e.g.

```
source graph:
|   |
v   V
MatMul
  |
  v

target graph:

|   |
v   V
partition partition
|   |
v   V
MatMul
  |
  V
reduce    X
  |       |
  V       V
  MatMul
    |
    V
```

然后这个新生成的reduce就可以reorder到其他地方, e.g.

```
source graph:

|   |
v   V
partition partition
|   |
v   V
MatMul
  |
  V
reduce    X
  |       |
  V       V
  MatMul
    |
    V

target graph:

|   |
v   V
partition partition
|   |
v   V
MatMul    X
  |       |
  V       V
  MatMul
    |
    V
  reduce
    |
    V
```

比如:

```cpp
GraphXfer *create_partition_linear_combine(FFModel *model, int num_dims,
                                           int num_parts, ActiMode activation,
                                           bool use_bias) {
  GraphXfer *subst = new GraphXfer(model);
  TensorX input = subst->new_tensor();
  OpX *linear1 = subst->create_linear(
      input, NULL /*matchOpX*/, num_dims, activation, use_bias);
  OpX *repartition = subst->create_repartition(input, num_dims - 2, num_parts);
  OpX *linear2 = subst->create_linear(repartition->outputs[0], linear1 /*matchOpX*/,
                                      num_dims, activation, use_bias);
  OpX *combine =
      subst->create_combine(linear2->outputs[0], num_dims - 2, num_parts);
  subst->map_output(linear1->outputs[0], combine->outputs[0]);
  // source graph
  subst->srcOps.push_back(linear1);

  // target graph
  subst->dstOps.push_back(repartition);
  subst->dstOps.push_back(linear2);
  subst->dstOps.push_back(combine);
  return subst;
}
```

> 学习一下Unity中的3对P变都是怎么使用的

TODO 好像只有partition和reduce呢


### TASO source code

入口: `graph.optimize [python] -> Graph::optimize [cpp]`

> 搞清楚怎么使用operator specifications/primitive operators

```cpp
std::vector<OpTemp*> ops;
ops.push_back(new MatmulTemp(AC_MODE_NONE));
operator_names[ops.back()] = "MatMul";
ops.push_back(new ElementTemp(OP_EW_ADD));
operator_names[ops.back()] = "EWAdd";
ops.push_back(new ElementTemp(OP_EW_MUL));
operator_names[ops.back()] = "EWMul";
ops.push_back(new Conv2DTemp(3, 3, 1, 1, true, false));
// ...
// 待优化子图, 用于测试的输入, opt spec
dfs(0, graph, inputs, ops, hashmap, transfers);
```

> 怎么无中生有自动生成子图替换? 搞清楚怎么枚举子图和可能的替换?

Generation Algorithm(S2.2):

好像主要在generator文件夹中, 而且是一个独立的程序生成一个子图替换文件`graph_subst.pb`, 然后在optimize中会读取该文件`load_graph_xfer_from_pb_file`, 主算法位置:

```cpp
// generator.cc::dfs
  // ...
  // 自动生成替换子图
  for (int i = 0; i < ops.size(); i++)
    switch (ops[i]->type) {
      case OP_EW_ADD:
      case OP_EW_MUL:
      {
        OpTemp* op = ops[i];
        for (int j = 0; j < inputs.size(); j++)
          for (int k = j + 1; k < inputs.size(); k++)
			// compute(input[j], input[j+1])
            if (op->compute(inputs[j], inputs[k], depth)) {
			  // 新操作的输出看成新子图的输入
			  // "生成树"
              inputs.push_back(op->outputs[0]);
              graph.push_op(op, inputs[j], inputs[k]);
              dfs(depth + 1, graph, inputs, ops, hashmap, transfers);
              graph.pop_op();
              inputs.pop_back();
            }
        break;
      }

```

定义怎么做生成

```cpp
// 待优化子图, 用于测试的输入, opt spec
dfs(0, graph, inputs, ops, hashmap, transfers);
```

> 怎么做剪枝优化搜索空间?

S4 Pruning Redundant Substitutions

> 怎么做正确性验证?

TODO:


### Whale: Efficient Giant Model Training over Heterogeneous GPUs

- 接口层(接入TF), 中间表达层(通过TaskGraph、VirtualDevices和策略抽象来表达各种并行策略), 并行引擎(探索计算图, 自动生成并行计算图), 执行引擎(转成TF graph让TF来执行)
- 通过replicate(数据并行)和split(流水并行)这两种并行化接口，可以表达出各种不同的并行化策略
- 计划器


## 2023-06-27

### Unity source code:

```
register_flexflow_internal_tasks 

FFModel::compile -> TaskLauncher launcher(GRAPH_OPTIMIZE_TASK_ID)
-> graph_optimize_task -> try_one_lambda -> graph_optimize 
-> GraphSearchHelper::graph_optimize -> generic_sequence_optimize
-> base_optimize(metaflow的pop和run)
```


#### QA

> PCG中的P操作怎么结合进子图替换里的??

**source graph的根据原图的op查的匹配, 但P变完成的无中生有的, 所以要和谁(什么模式)match?**

看`create_reduction()`等的run的代码

TODO: next day


### taccl source code


### taccl

状态机step by step执行, 集合通信转换成一系列p2p通信, 用一个解释器kernel来执行。

- terms
    * Instances, threadblock执行的数量, 以用满bw


#### QA

> 如何自动生成算法并保证正确性?

S5 TACCL Synthesizer & B TACCL Synthesizer in Detail

- 问题定义
    1. 每个GPU数据分成C个chunk
    2. 看成mixed integer linear program (MILP)问题(NP-hard), 使用[Gurobi]来处理
        - 每个chunk有个`start_time`变量标注chunk什么时候可用
        - `is_sent`标注(chunk, link)是否chunk从link发送了数据
        - `send_time`标注chunk何时发送的
        - 加点带宽的正确性的限制
        - 问题就变成了最小化所有传输中耗时最大的时间
    3. taccl还会自动合并多次的传输

- routing
- ordering
- contiguity and exact scheduling

TODO:

TODO: 如何生成

Appendix B.


TODO: 如何正确


> TACCL(msccl)居然还有异步优化, 怎么做的!?? 如果可以异步那overlap就简单多了

"TACCL transforms a synthesized algorithm into the asynchronous execution model"


## 2023-06-26

### taccl

- 找最优解NP-hard, 那就引入人在回路(通信草图), 来联合搜索
- 提供一个physical topology profiler来方便用户知道实际的拓扑(尤其是在云上)
- 代价模型: a + s/b, 
    * a: latency, 发送1bit的延迟
    * b: bandwidth
    * s: 数据量


#### QA

> 通信草图的原理的定义? TODO

四要素

- Logical Topology
    * 无视节点间相连设备NIC等
- Switch-Hyperedges
    * switch可以让节点间直接p2p, 但随着连接数的增多, 延迟增多。数据量大, 连接又多导致的拥挤
    * taccl利用Switch-Hyperedges控制一个switch的连接数, 一个超边直接表示成逻辑拓扑上的一条p2p边
- Algorithm Symmetry
    * ?????? TODO
    * 集合通信算法有一定对称性: e.g. 有收就有发
- Input size


> 怎么保证自动生成的算法是正确的? TODO

Algorithm Symmetry??????? TODO


> taccl怎么使用通信草图?

TODO:

> 生成的算法怎么接入nccl?

同msccl, 生成taccl-ef文件, 然后可以用环境变量传入`MSCCL_XML_FILES=<taccl-ef>`

TACCL synthesizes a collective algorithm by deciding the route that each data chunk in the
collective should take in the topology as well as the ordering of chunks at every link

本质道理是任何集合通信算法都可以用trivially的p2p算法实现, 所以扩展NCCL，加入一个interpreter, (MSCCL, TACCL runtime)


### Transformations to Parallel Codes for Communication-Computation Overlap

- A key aspect of the program transformation consists of replacing a collective operation with a set of point to point asynchronous
    * 现成的集合通信替换成p2p异步通信

#### QA

> 怎么自动生成overlap的代码?


### Unity

- 子图替换生成算法
    1. 根据用户给定的基础操作集DFS枚举子图, 收集指纹(output的哈希值)
        - 多输出的情况怎么哈希? 两步哈希
        - **枚举输入tensor, 如果可以输入新operator则插入op, 并将op的输出插入到输入tensor集**
    2. 根据fingerprint初筛


#### QA

> 如何使用operator properties来形式化验证?

手动写一系列operator properties(原理类似混合器, 模式匹配然后替换判断?)。operator表示成函数

TODO:








