# idea

- 子图替换友好的变成语言
    * 用户写naive的代码但可以被编译成高性能的代码: 如融合内存访问成一次等
- idea
    * [x] relax的代价模型给taccl(已经用上了)
- 人为/自动局部性, 局部性aware

- **intra的时候nvlink aware, 本来用pcie跑的移动到GPU上用GPU跑**
    * 至于往GPU上拷贝的问题, 那可以做一个GPU内存管理
    * 但什么场景会有intra CPU呢? byteps...
- GPU内的数据库(一种新的AI范式) => 做intra的时候能用专线尽量用专线(nvlink), 及时可能有些tradeoff
    * 用SQL语句类似的语言DSL描述算子
    * Model就等于一个SQL套SQL
    * 然后利用SQL结构化的特点就可以做编译优化: 执行计划, 优化器, 执行器
    * 关系代数 => 交换律, 结合律等??
        + 但是和直接写pytorch模型没有区别........
- 考虑类似NUMA aware的解法
    * machine map/device map/proxy opt
- pci aware + nvlink aware的无限zoom out机
    * 不断看成子系统

- **多跳NVlink可以比一次PCI好**
    * 现在的all reduce这些算法是不是存在这个问题? 比如只有一个NIC
    * 整一个**GPU内的pipeline**, 专门负责利用多跳NVlink传输数据
        + 而且bw(NVLink) > bw(NIC), 流水线一定是可以填满的
- GPU内的流水线
    * device map
    * topo aware but also 透明(所有GPU看成一个进行编程)

- **GPU sub system as server**

- one fit all只有编译器能近似做到了
    * 需要一种语言

- PS 出现的时候分布式训练还没兴起, 分布式训练的研究对于分布式系统的研究就是小cases
    * 有没有可能同理现在DB, 网络也有可套用的

- 参数服务器和非参数服务器是否可以抽象成同一形式??

- **GPU - CPU 流水线一体化**
    * 处理NIC异构的情况
    * NVLink不全连接的情况
    * **pipeline ordering**

- 流水线重构
    * GPU-CPU流水线一体化: 处理NIC异构的情况(拓扑感知)
        + ref
            + ? Logical/Physical Topology-Aware Collective Communication in Deep Learning Training
        + 终点发往NIC底下的卡, NVLink直连的卡等, p2p ordering?
    * 压缩: 计算换传输
        + ref
            + Gradient Compression Supercharged High-Performance Data Parallel DNN Training
            + ZeRO++
    * 气泡填充: 额外任务/任务交叠
        + ref
            + PIPEFISHER: EFFICIENT TRAINING OF LARGE LANGUAGE MODELS USING PIPELINING AND FISHER INFORMATION MATRICES
        + idea
            + 压缩的额外任务
            + 优化器的额外任务
            + prefetch
            + ZeRO+Magatron, os+g+p的额外任务
    * ABS
        + task manager/scheduler, dependency and hidden currency
        + multiple queue, for better overlap
            + 用于和调度器交互
            + main queue
            + extra queue, Unused
        + device manager/device map: 异构, nic pin, mpress..
    * misc
        + token处做流水线(TeraPipe) + mpress
        + 全双工/半双工导致的气泡问题
            + 显然没有这个问题, 不然人家实验就发现了

- 这3D并行他均匀吗?
    * 即dp, tp时是否存在空闲

- [x] ZeRO pipeline有没有可能
    - [x] 已有, build-in: prefetch
    * comm和comp overlap
    * 算一半的时候传另一半
        + N个GPU, 模型切2N份, 每个GPU保存2份

```
comp: -
comm: =
----========

    --  --
========

comp = a, comm = b, a + b = 100%

org:
a + b = 100

overlap(流起来后)
b/2 + a/2 => 最快50%
```


主要针对流水线并行这一方向考虑如何优化和参与代码重构

1. 分析taccl源代码中执行器部分的实现, 使用的是msccl解释执行, 本质是集合通信算法由若干简单的p2p算子组成
2. 熟悉项目代码，保留megatron和deepspeed中的流水线的实现，方法后续对代码的重构
3. 定量分析megatron和deepspeed中的数据通信量, 估计后续优化的效果
4. 受byteps中一个讨论的启发, 想到可以对流水线顺序进行调整，让最后一级流水线尽可能靠近网卡所在PCI switch，这样在异构环境下，涉及到跨机流水的部分就可以加速，优化掉跨PCI switch的开销
5. 受pipefisher和hipress启发, 流水并行优化主要在于气泡填充，因此可以在气泡中插入不依赖或少依赖前向和后向的操作，如优化器额外状态的计算，数据压缩等。可以使用双任务队列的流水线模式，主队列处理前向和后项的任务, 次队列处理额外任务



## 2023-07-07

### Reducing Activation Recomputation in Large Transformer Models

论点: 通过减少重算来加速训练。因为激活值的存储需要空间, 所以常会引用重算(full recompu)

- 主要idea: 分布式激活值
- observe: 30-40% 的重算overhead
- Sequence Parallelism可以减少激活值的显存占用
    * Sequence Parallelism: 即输入也切分
    * 并且通过分析发现tensor并行和sequence并行可以结合(ring allreduce = reduce-scatter + allgather), 导致不引入额外通信开销

- 流水并行也需要降低激活值存储
- selective recompute
    * mem和comp的tradeoff, 只重算"计算量小但产生激活值多"的操作

- 激活值内存分析: 设输入s x b x h, b表示micro batchsize
    * 激活值定义: 前向计算阶段计算的, 在反向传播中用到的值
    * attn block: attn + linear + dropout 
        + attn: TODO: the paper
        + linear: 2sbh
        + drop: sbh的mask


- 各种并行的激活值分析
    * tp:
        + 输入不并行, 值并行了attn和linear, Figure 4
    * Sequence Parallelism
        + laynorm和dropout部分的操作和seq的维度无关, 可以按着seq切分
        + 相当于在这部分做了tp, 即所谓的s p, Figure 5
        + 而有分就有和, 需要引入额外的聚合操作, 这些聚合操作和tp是可以融合的从而不引入额外开销


#### QA

##### 为什么tensor并行是Figure 4所示呢??

即为何layernorm, dropout不包含在内??

因为这些操作计算量不大。

**但是产生的激活值不小**: Equation 2的10sbh



##### tensor并行和seq并行的区别

tensor并行指切模型不切输入, seq并行即切模型又切输入?? Figure 5

```
tensor
a11, ___       b11, b12        a11*b11 + 0,  a11*b12 + 0
           x              =   
a21, ___       ___, ___        a21*b11 + 0,  a21*b12 + 0


a11, a12       b11, ___        a11*b11 + a12*b21,
           x              =   
a21, a22       b21, ___        a21*b11 + a22*b21,
=> [2, 2] x [2, 1] => [2, 1]
```



### pipeline proof

#### DDP

> 主要就是all reduce: 2(N-1)/N * B

- all reduce: 2(N - 1)/N * B
- intra + inter all reduce:
- ZeRO++ all reduce:

- Deepspeed: (2 + 2 + K) * S
    * 流程
        1. fwd: all-gather得到完整权重
            + broadcast param
        2. bwd: all-gather
            + broadcast os
            + reduce grad
    * stage1: 每份3/N * B, N个GPU就是3B
    * stage2: 每份2/N * B, N个GPU就是2B
    * stage3: 每份2/N * B, N个GPU就是2B
- Magatron
    * 每层4次all reduce
    * 每次reduce通信量2(N-1)/N * B
    * 参数量 = 输入 = h*s
    * 一共bs个micro batch => 总每次数据量 = bs * h * s * sizeof(16FP)


- 异构, 同构, 针对一个GPU

##### QA

> ZeRO比megatron快？ZeRO通信量似乎更低

#### pipeline

- Deepspeed
- Magatron


## 2023-07-06

### pipeline theory proof

- intra GPU(GPU MEM)
- inter GPU(PCI/NVLink)
- intra CPU(CPU MEM)
- inter CPU(PCI)
- intra Node(CPU MEM/PCI)
- inter Node(NIC/IB)

定量(特定时间要什么设备)


定性(特定设备有什么时间)


#### 数据量估计

TODO:

- l = 12
- s = 512
- h = 768
- a = 12

- 1T = 10^12

- V = 

训练: 96slh^2(B)

- h(hidden size)
- s(seq_len)

- QA
    * transformer的参数量为什么会有分母?
        + 因为是数学建模然后拟合出来的, 不一一对应transformer
    * 白皮书中的 "迭代步时长" 怎么获得?


#### transformer 参数计算

attn:

- qkv => qw, kw, vw, ww => 4 h^2
    * 其中w的形状为`[h, h]`
- 偏执h, 分别加到q k v w, 其形状为`[h]`, 总的4h
- 总的 = 4 h^2 + 4h

mlp: linear + bias

```
[h, h] * [h, 4h] + [4h]
            w       wb
[h, 4h] * [4h, h] + [h]
   w                wb
```

总的参数量就是4h*h + 4h + 4h*h + h = 8h^2 + 5h

layer norm: attn, mlp各一个。layer norm中有一个缩放和一个平移矩阵, 形状都是`[h]`, 总的2h * 2 = 4h

总4 h^2 + 4h + 8h^2 + 5h + 3h = 12 h^2 + 13h


#### ring all reduce通信量计算

- reduce-scatter
    * N个同时进行, 每个做B/N, 做(N-1)次 => (N-1)*B/N
- all-gather
    * N个同时进行, 每个做B/N, 做(N-1)次 => (N-1)*B/N

=> 2(N - 1) / N

> Megatron中每层要有2 * 2次allreduce(2层, 每层fwd bwd两次)


#### Pipeline通信量计算

- Q: 白皮书中的 "迭代步时长" 怎么获得? 循环论证
- Q: 白皮书中好像不考虑全全双工的情况

输入a(即s*h, embedding后的seq), 输出等于输入(transformer做词嵌入后重复N词, 所有输出等于"输入"), 传递到下层: s * h * sizeof(type)

如果的半双工还得bw * 2 ?

白皮书的问题: 主要是解决想要达到人家的水平, 那我们的配置至少得怎样。

我的建模:


#### ZeRO 内存计算

1. fwd: all-gather得到完整权重
    - broadcast param
2. bwd: all-gather
    - broadcast os
    - reduce grad

- S: 参数量
- None: (2 + 2 + K) * S
    * 2 = 2B = FP16的参数
    * 2 = 2B = FP16的梯度
    * K = KB = 优化器内存
- stage1: 优化器分区, aka `P_{os}`
    * 设N个GPU则(2 + 2 + K/N) * S
    * 前向: 参数和梯度完整, 不需要额外操作
    * 反向: TODO:
- stage2: stage1 + 梯度分区, aka `P_{os + g}`
    * 设N个GPU则(2 + 2/N + K/N) * S
- stage3: stage2 + 参数分区, aka `P_{os + g + p}`
    * 设N个GPU则(2/N + 2/N + K/N) * S



### deepspeed source code

> [ref](https://zhuanlan.zhihu.com/p/576673548)


- idea
    * "Pipeline parallelism is not compatible with ZeRO-2 and ZeRO-3"有办法插入么


#### module.py

- Usage
    * `initialize()` -> PipelineEngine -> `train_batch()` -> scheduler.TrainSchedule(PipeSchedule.__next__) -> steps -> PipelineModule
- `__init__.py`
    * 定义`_layer_specs`, 接收一组layer, 方便后期分配到流水线上
    * 定义`forward_funcs`, 
    * 添加任务队列/layer队列到`forward_funcs`

#### QA

##### 在哪做P2P?


##### 状态机在哪?

没有状态机那大家都执行同一份代码怎么知道自己是接收还是发送?



TODO: PipelineParallelGrid()

### TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models

- idea
    * **可以利用mpress的设备调度机制!!!**
    * 然后我记得GPU是没有预取的(from MSRA)

切分sequence length维度，即使长时序的单个sample也可以做流水并行, 气泡更小。



## 2023-07-05

### Gradient Compression Supercharged High-Performance Data Parallel DNN Training

论点: 压缩和传输没有协同设计, 插入压缩算法对DNN开发者不友好

[HiPress](https://gitlab.com/hipress/hipress)

- CaSync灵活组合计算和传输的原语 => comp, commu pipeline
    * Compression-Aware Synchronization
    * 压缩的计算开销的很大的, 会稀释对传输的优化
- CompLL方便开发者, 因为要写GPU代码, 不好写, 所以CompLL


##### 如何decouple 梯度同步原语的?

1. 抽象出通信拓扑图: 边是连通关系, 点的计算节点
    - 两类节点: worker, aggregator
        * worker计算梯度, aggregator收集梯度
2. 流程拆解: encode, decode, merge, send and recv
    - 在哪可以做aware优化? 就是拆开后可以更灵活见缝插针
    - 好像只有PS环境好用, 因为只有merge可以出现岔路??

TODO

##### 协同在哪？在哪对流水线友好了？

就是拆开后可以更灵活见缝插针。抽象一个task manager来管理依赖关系等, 然后push到两个独立的任务队列。

> As shown in Figure 2, at Step

有没有更直观的示例?


### PIPEFISHER: EFFICIENT TRAINING OF LARGE LANGUAGE MODELS USING PIPELINING AND FISHER INFORMATION MATRICES

- 添加K-FAC优化优化任务到流水线中
    * K-FAC本身可以拆分成多步: curvature, inverse, and precondition等
- **AutomaticWork Assignments**
    * profile执行时间
    * K-FAC任务**队列**中根据rule见缝插针
    * K-FAC排期完后就可以用这样的固定顺序了


#### QA

##### 为什么可以塞？不用等梯度算完才做优化吗？

因为出了真正用的梯度的地方, 还可能有梯度independent的运算, 这部分可以先算, 或者说用来填充流水线。


##### 如何填充气泡？分配流水线任务？

### Logical/Physical Topology-Aware Collective Communication in Deep Learning Training


## 2023-07-04

### ps lite

### byteps source code

- scheduler
    * reorder
        + 一次发一批, 这批的窗口内不可重排, 但窗口外的可以重重排
    * (S3.3) Interaction with Framework Engines: dependency proxy
        + 问题: 不同框架执行引擎不同, 有的用计算图, 有的用FIFO。需要想办法保证正确性, 还要处理同步异步并行的情况。
    * barrier问题, 有些框架有barrier抽象, 导致需要等待
        + bp中通过插入异步Op来解决等待问题(先干其他事): 后台执行立刻返回
        + Figure 8: 全局的等待分解成partial的等待
    * 自动切分到合适大小的传输块
- byteps paper 2

#### QA

##### 在哪做的CS, SS?

TODO

##### 在哪做的pci-only, nvlink-based的aware?

- pci-only: CPU-assisted aggregation
- nvlink-based: reduce + broadcast

TODO

## 2023-07-03

### Taccl source code

#### routing

> `routing.py`


#### ordering

> `heuristic_ordering.py`

#### contiguity and exact scheduling

> the aim of this stage is to find the sweet spot in the trade-off between lower link latency and reduced pipelining benefits

### Taccl Appendix B

- terms:
    * coll(collective): the algo
    * C: chunks
    * R: rank
    * (c, r): 数据存在的情况
    * (r1, r2): 链路存在的情况
        + logical topology: topology和草图
    * S_send(r): r -> switch的情况
    * S_recv(r): switch -> r的情况
    * a(r1, r2), b(r1, r2): cost model
    * lat(r1, r2): a(r1, r2) + b(r1, r2)
- Usage
    * 

#### routing

> The main aim of the routing stage is to give us the path that every chunk takes in the collective.

- `start[c, r]`c的就绪时间
- `send[c, src, r]`src -> r的传输时间
- 一个(c, r)在
- 各种约束
    * 时序前后的约束
    * switch
    * relaex constrain


#### ordering

> We start the heuristic ordering by determining the paths each chunk takes using the solution of the path encoding.


- chunk-with-shortestpath-until-now-first
    * 在此之前选择最短路径的块
- chunk-with-longest-path-from-now-first
    * 在此之后选择最长路径的块
- link time: 最早link可用时间
- chunk time: 最早chunk可传输时间
- algo
    1. all link time = chunk time = 0
    2. first round:
        1. 选 a path and associated chunk, (p, c)
        2. schedule (p, c) 走path的第一个link
        3. link time和chunk time都加速link latency
        4. 选择path中的下一个link
    3. next round:
        1. 根据tracked link和chunk time选择下一个link
        2. 知道所有link都有数据传输



#### contiguity and exact scheduling



### ZeRO ++

主要说明了可以通过压缩数据量来得到不错的优化。comp commu overlap不单单局限于实际模型的comp, 也可以考虑到其他comp(如压缩)

> ZeRO++ 中额外的优化
>
> 基础优化: 编码压缩, 局部缓存(分层replica), all2all优化ring

- abs(读码tips): 
    * 一个压缩(quantization)库
    * 访问本地缓存
    * all2all算法的实现
    * intra, inter流水线的实现
    * 人工优化的kernel的实现

- Overlap Compute and Communication
    * 编码和传输overlap
        + 分析执行流程
        + 编写异步代码
    * intra, inter overlap, 流水线化
        + tensor ordering可以细化成多流水线, 存在通用算法
- 优化kernel, 包括编码解码, tensor ordering, kernel融合
    * 用满mem bw, 通过人工融合降低traffic
    * Maximizing Bandwidth Utilization: 
        + 向量化 + 并行指令
    * Minimizing Total Traffic:
        + 硬件友好, cache友好的块大小
        + 融合tensor reshape和quantization, 优化内存访问
        + 融合解码和reduction, 优化内存访问


#### QA

##### 异步虽异步, 那依赖关系怎么处理?

主要是说当前层参数的fetch和向下一层的编码可以同时执行。依赖处理方面会等待编码完成(ZeRO++ synchronizes the quantization stream)









