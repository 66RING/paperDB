# idea

- 切分大小不均问题滴有??
    * 流水线并行等切分应该没问题, 因为现在模型都是重复堆叠的多(transformer)
    * tensor并行和数据并行呢?
- if compiler(DSL) for nccl, what if compiler(DSL) for 算子
- ccl自动chunk大小

python调试:

```
python -m pdb <you_file.py> <args>
```

- ps: ipdb有高亮
- 设置断点: 代码中插入`import ipdb; ipdb.set_trace()`


## 2023-06-16

### byteps paper **2**: A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters

> GPU CPU NIC异构的利用率。处理GPU外还希望利用好CPU和带宽资源

- GPU数量固定的情况下也尽可能高的利用CPU
- Summation Service(SS)抽象
    * 全部优化放CPU可能慢，但sum等cpu强的用cpu跑，优化器拆开跑可能不慢
    * run on both CPU and GPU machine
    * SS功能简单: sum up and send back
    * 任务分配分析: SS4.1.1 TODO
- 额外考虑的跨pci switch子系统的情况(intra node的情况)
    * "hierarchical all-reduce"再优化, 额外考虑intra内的跨switch的情况
    * PCI only: 问题是没有考虑跨switch的情况
        + CPU-assisted aggregation：
            1. 同switch下l个GPU先本地做sum(reduce-scatter), 每个GPU就有M/l的数据
            2. GPU-CPU copy: l个GPU的M/l的数据拷贝回CPU, 每个swtich就有M的数据
            3. CPU-reduce: 加和每个switch的M数据
            4. Networking: CS, SS通信
            5. CPU-GPU copy: M/l拷贝回GPU
            6. all-gather: 同switch内GPU做all-gather
            - 相当于单机处理的M数据多机处理了
    * NVLink based: 问题是NIC只和一个switch相连, 导致topo不对称, 需要特别关注nic的位置。然后CS, SS同时使用导致成为瓶颈 -> 想办法分开
        + 用reduce + broadcast替代reduce-scatter + all-gather, 来**让出NIC的带宽**
            + 可以这么做是因为NVlink太快了, **多跳NVlink可以比一次PCI好**
            + 做好路线规划

- 但是一台机器上CPU GPU同时使用就说明一个机器即做worker又做server, 从而导致单台数据量翻倍
    * byteps需要在其中寻找平衡

- 原PS设计的瓶颈: SS6.2 RDMA performance


### coconet中的overlap

低版本nccl命名: `ncclAllReduceRingKernel`

- 搞清楚allreduce算法, 才能知道怎么overlap
    * allreduce
        1. 对于`process[p]`会发送`chunk[p]`到下一个process, 同时接受来自上一个`process的chunk[p - 1]`, 这样一轮下来一个chunk就reduce了2个rank, 做nrank轮就reduce-scatter了
        2. 再来一轮ring all-gather: reduce好的chuck发送给下一个process的下一个chunk, 执行np次


### msccl source p2

- 带着问题看
    1. 调用哪个算法由谁决定
        - 添加标识符后都通过`runInterpreter()`同一解释
    2. 为什么ngpu回变化
        - TODO: 1. 因为每个节点都要执行相同的代码
    3. 算法怎么应用上nccl的
        - 用什么机制调用到`runInterpreter`的??
        - IMPL_COLL -> NCCL_KERN_NAME() -> __device__ ncclKernel -> struct RunWorkMSCCL -> run -> RunWorkElement<Fn, T, RedOp, NCCL_ALGO_MSCCL, NCCL_PROTO_LL>() -> runInterpreter -> prims_xxx

#### 通过nccl中ring算法的实现理解怎么使用nccl原语。

> runRing()

构建prims对象

```cpp
__device__ Primitives(
    const int tid, const int nthreads, int const *recvPeers, int const *sendPeers,
    void const *inputBuf, void *outputBuf, uint64_t redOpArg, int group=0
):

Primitives<T, RedOp, FanSymmetric<1>, 1, Proto, 0> prims
      (tid, nthreads, &ring->prev, &ring->next, args->sendbuff, args->recvbuff, args->redOpArg);
```

几步走

```cpp
// step 0: push data to next GPU
chunk = modRanks(ringIx + nranks-1);
offset = calcOffset(chunk);
nelem = min(realChunkSize, size-offset);
prims.send(offset, nelem);

// k-2 steps: reduce and copy to next GPU
for (int j=2; j<nranks; ++j) {
  chunk = modRanks(ringIx + nranks-j);
  offset = calcOffset(chunk);
  nelem = min(realChunkSize, size-offset);
  prims.recvReduceSend(offset, nelem);
}

// step k-1: reduce this buffer and data, which will produce the final
// result that we store in this data and push to the next GPU
chunk = ringIx + 0;
offset = calcOffset(chunk);
nelem = min(realChunkSize, size-offset);
prims.directRecvReduceCopySend(offset, offset, offset, nelem, /*postOp=*/true);

// k-2 steps: copy to next GPU
for (int j=1; j<nranks-1; ++j) {
  chunk = modRanks(ringIx + nranks-j);
  offset = calcOffset(chunk);
  nelem = min(realChunkSize, size-offset);
  prims.directRecvCopySend(offset, offset, nelem);
}

// Make final copy from buffer to dest.
chunk = modRanks(ringIx + 1);
offset = calcOffset(chunk);
nelem = min(realChunkSize, size-offset);
prims.directRecv(offset, nelem);
```


## 2023-06-15

### nccl source code

> [拓扑识别和图构建](https://blog.csdn.net/KIDGIN7439/article/details/126990961)

- **ncclTopoGetSystem** 拓扑分析
    * 本质: 找到路径
    * 表示成xml树
    * 注意就是读系统的拓扑, 父子关系, 找到路径
- 建图?? TODO: 建什么图, 用来干嘛
    * 为了路径搜索: v, e, w
- 路径搜索, ncclTopoComputePaths

TODO:

### msccl source code

> msccl实现了更细粒度的原语, 配置解释器来执行

- 带着问题看
    1. 调用哪个算法由谁决定
        - 添加标识符后都通过`runInterpreter()`同一解释
    2. 为什么ngpu回变化
        - TODO: 1. 因为每个节点都要执行相同的代码
    3. 算法怎么应用上nccl的
        - 用什么机制调用到`runInterpreter`的??
        - IMPL_COLL -> NCCL_KERN_NAME() -> __device__ ncclKernel -> struct RunWorkMSCCL -> run -> RunWorkElement<Fn, T, RedOp, NCCL_ALGO_MSCCL, NCCL_PROTO_LL>() -> prims_xxx

- initTransportsRank
    * 读取算法xml文件: init.cc `mscclGetAllAlgoFromXMLFilesAndSetInfo`, 解析`MSCCL_XML_FILES`指定的每个文件, 最多4个算法
    * 解析xml中标注的属性, 保存到`mscclInfo->mscclDevComm.mscclAlgos[algo_offset]`中
    * 算法的解析在`mscclGetAllAlgoFromXMLFilesAndSetInfo()`中约有300loc的parse
        + 解析每个gpu的`nScratchChunks, nInputChunks, nOutputChunks`
        + 如果xml中描述的id和当前gpu匹配, `id == rank`则继续解析每个tb的行为
            1. 解析tb的send/recv的peer
            2. 解析tb的每个step: `s(step_idx), srcoffset, dstoffset, depend_bid, depend_step, has_dependence, count, srcbuffer, dstbuffer, type`
                a. type: step的类型recv, send, `MSCCL_LOCAL_COPY`等
            3. mscclGetBufferType解析是输入还是输出buffer
            4. TODO: 后半段没看懂, 1099
            5. 也就是记录一些信息, 还不知道怎么用
    * 解析的结果保存在`mscclinfo = &comm->mscclHostComm`中
    * connect MSCCL connections
        + 好像只是建立连接之类的, 没具体使用 TODO
- ncclSetupAsyncKernels 
    * computeColl, 计算参数
    * `ncclKerns[funcIndex]`找到函数
    * 调用mscclWork.run() -> runInterpreter
    * 通过`extern ncclShmemData ncclShmem`传递参数

- reduce等仍是原理的reduce, 就是参数不同?? **不是**, 给了MSCCL的入口

```cpp
template<typename T, typename RedOp>
struct RunWorkElement<ncclFuncAllReduce, T, RedOp, NCCL_ALGO_MSCCL, NCCL_PROTO_SIMPLE> {
  __device__ __forceinline__ void run(ncclWorkElem *args) {
    using Proto = ProtoSimple<MSCCL_CHUNKSTEPS/MSCCL_SLICESTEPS, MSCCL_SLICESTEPS>;
    runInterpreter<T, RedOp, Proto>(args, 1);
  }
};

template<typename T, typename RedOp>
struct RunWorkElement<ncclFuncAllReduce, T, RedOp, NCCL_ALGO_MSCCL, NCCL_PROTO_LL128> {
  __device__ __forceinline__ void run(ncclWorkElem *args) {
    runInterpreter<T, RedOp, ProtoLL128>(args, 1);
  }
};

template<typename T, typename RedOp>
struct RunWorkElement<ncclFuncAllReduce, T, RedOp, NCCL_ALGO_MSCCL, NCCL_PROTO_LL> {
  __device__ __forceinline__ void run(ncclWorkElem *args) {
    runInterpreter<T, RedOp, ProtoLL>(args, 1);
  }
};
```

- ps:
    * nccl也是直接使用驱动api的比如launchKernel


## 2023-06-14

### megatron 

#### impl

> ref: https://zhuanlan.zhihu.com/p/407094090, https://zhuanlan.zhihu.com/p/388830967
>
> 回答核心问题: 有2台机器，每台机器8卡，一共16张卡；在tensor-parallel-size=2, pipeline-parallel-size=4的时候，每个模型的“子块”是如何被分配到16张卡上的？

**只需要搞清楚两个要点: 组怎么划分, 组间/组内需要什么交互。** TODO

怎么划分, p 4, t 2, d 2为例:

```
p 4, t 2, d 2
pg = 16/4 = 4
tg = 16/2 = 2
设2node, 16gpu: [g0, ..., g15]
node 1: [g0, ... g7]
node 2: [g8, ... g15]

d g:
[0 1 2 3] [4 5 6 7] [8 9 10 11] [12 13 14 15] => 
[0 2] [1 3] [4 6] [5 7] [8 10] [9 11] [12 14] [13 15]

m g
[0 2] [1 3] [4 6] [5 7] [8 10] [9 11] [12 14] [13 15]
[0 1 4 5 8 9 12 13] [2 3 6 7 10 11 14 15]

t g
[0 1] [2 3] [4 5] [6 7] [8 9] [10 11] [12 13] [14 15]

p g
[0 4 8 12]
[1 5 9 13]
[2 6 10 14]
[3 7 11 15]
```

具体交互内容(看哪里用了这些组, 用来干什么) **NOTE:**

- tensor group
    * 列切tensor: ColumnParallelLinear
        + 准备: all reduce梯度: `copy_to_tensor_model_parallel_region` -> `all_reduce`
        + 开乘: `linear_with_grad_accumulation_and_async_allreduce` -> LinearWithGradAccumulationAndAsyncCommunication -> allgather
- data
    * fetch?? TODO
    * hook累加梯度??
- pipeline group
    * p2p?? TODO


- pipeline parallel among node
- tensor parallel inside node

- 流水线并行
    * `p2p_communication.py::_communicate`
        + p2p, 阻塞send/recv, 状态机
- PipeDream实现: `schedule.py::forward_backward_pipelining_without_interleaving`
    * warm-up phase: 启动状态，每个worker (除了最后一个stage的) 只做forward计算和向下游发送intermediate activation，直到最后一个stage被触发.
    * steady phase: 稳定状态，每个worker开始规律地进行one-forward-one-backward (1F1B).
    * cooldown phase: 结束状态，将剩下的in-flight的microbatches执行完，只有backward计算和向上游发送intermediate gradient w.r.t activation.

- `model/distributed.py`
    * We only support local DDP with multiple micro-batches.
        + local DDP没有通信和计算的overlap

数据并行的all reduce

```python
# All-reduce if needed.
if args.DDP_impl == 'local':
    timers('backward-params-all-reduce').start()
    for model_module in model:
        model_module.allreduce_gradients()
    timers('backward-params-all-reduce').stop()
```



#### parallel group

- 分组: 本质就是range切分, n张卡, (p, t, d)表示流水线, tensor, 数据并行度
    * data p groups
        + 先n / p分大组(和pipeline group对应), 然后组内等间隔t取值分data p小组
    * tensor p groups
        + 均分成一个个区间即可
        + rank一定相邻
    * pipeline p group
        + 也是均分成一个个区间
    * 总体的, 从overview看:
        + 先是数据并行分组, 多个显卡处理一个数据切分, 看成整体的一块显卡
        + 数据切分组内, 在用流水线并行的方式处理这个整体是任务
        + **至于tensor并行就比较微妙**, 它的rank和数据并行分组应该满足某种倍数关系, 否则就浪费??笔者不太确定, **NOTE: 非常微妙**
            + 比如16卡的(p, t, d) = (4, 2, 2), 一个流水并行内就可以再分多个tensor并行???

```python
data_parallel_size: int = world_size // (tensor_model_parallel_size *
                                         pipeline_model_parallel_size)

num_tensor_model_parallel_groups: int  = world_size // tensor_model_parallel_size
num_pipeline_model_parallel_groups: int = world_size // pipeline_model_parallel_size

# Build the data-parallel groups.
global _DATA_PARALLEL_GROUP
global _DATA_PARALLEL_GLOBAL_RANKS
assert _DATA_PARALLEL_GROUP is None, 'data parallel group is already initialized'
all_data_parallel_group_ranks = []
for i in range(pipeline_model_parallel_size):
    start_rank = i * num_pipeline_model_parallel_groups
    end_rank = (i + 1) * num_pipeline_model_parallel_groups
    for j in range(tensor_model_parallel_size):
        ranks = range(start_rank + j, end_rank, tensor_model_parallel_size)
        all_data_parallel_group_ranks.append(list(ranks))
        group = torch.distributed.new_group(ranks)
        if rank in ranks:
            _DATA_PARALLEL_GROUP = group
            _DATA_PARALLEL_GLOBAL_RANKS = ranks

# Build the model-parallel groups.
global _MODEL_PARALLEL_GROUP
assert _MODEL_PARALLEL_GROUP is None, 'model parallel group is already initialized'
for i in range(data_parallel_size):
    ranks = [data_parallel_group_ranks[i]
             for data_parallel_group_ranks in all_data_parallel_group_ranks]
    group = torch.distributed.new_group(ranks)
    if rank in ranks:
        _MODEL_PARALLEL_GROUP = group

# Build the tensor model-parallel groups.
global _TENSOR_MODEL_PARALLEL_GROUP
assert _TENSOR_MODEL_PARALLEL_GROUP is None, \
    'tensor model parallel group is already initialized'
for i in range(num_tensor_model_parallel_groups):
    ranks = range(i * tensor_model_parallel_size,
                  (i + 1) * tensor_model_parallel_size)
    group = torch.distributed.new_group(ranks)
    if rank in ranks:
        _TENSOR_MODEL_PARALLEL_GROUP = group

# Build the pipeline model-parallel groups and embedding groups
# (和Build the model-parallel groups等价)
# (first and last rank in each pipeline model-parallel group).
for i in range(num_pipeline_model_parallel_groups):
    ranks = range(i, world_size, num_pipeline_model_parallel_groups)
    group = torch.distributed.new_group(ranks)
    if rank in ranks:
        _PIPELINE_MODEL_PARALLEL_GROUP = group
        _PIPELINE_GLOBAL_RANKS = ranks
    # Setup embedding group (to exchange gradients between
    # first and last stages).
    if len(ranks) > 1:
        embedding_ranks = [ranks[0], ranks[-1]]
        position_embedding_ranks = [ranks[0]]
        if pipeline_model_parallel_split_rank is not None:
            if ranks[pipeline_model_parallel_split_rank] not in embedding_ranks:
                embedding_ranks = [ranks[0],
                                   ranks[pipeline_model_parallel_split_rank],
                                   ranks[-1]]
            if ranks[pipeline_model_parallel_split_rank] not in position_embedding_ranks:
                position_embedding_ranks = [ranks[0],
                                   ranks[pipeline_model_parallel_split_rank]]
    else:
        embedding_ranks = ranks
        position_embedding_ranks = ranks

    group = torch.distributed.new_group(embedding_ranks)
```


## 2023-06-13

### flash attention

> 主打io aware, 快1.5~3x

- 局限性
    * 需要重写cuda kernel, 且cuda kernel可能不可跨框架移植
        + 可以高层语言写(作为接口), 通过编译器自动生成cuda代码
    * io aware在更多地方(除了attention), 虽然attention是最内存密集的
    * 多GPU io aware, 当前是单GPU
        + 又会收到很多并行等因素的影响

传统attention: 

1. S = QK, read QK, store S
2. P = softmax(S), read S, store P
3. O = PV, read PV, store O

- 方法: 尽量使用GPU的SRAM, ref: 非常详细且通俗易懂的[FlashAttention简记](https://zhuanlan.zhihu.com/p/582606847)
    1. tiling: 输入拆分成多块, 然后多轮计算, softmax的公式拆分成增量的形式
    2. recompute: 反向传播不存储注意力矩阵(S, P)
        - 存储一个softmax的归一化因子, 可以用来重算这两个矩阵. 
        - 怎么做到的? 论文S3.1


### FasterTransformer

#### in short(from zhihu)

> FasterTransformer提供了CUDA Kernel优化，显存优化，模型并行，多框架后端的功能



## 2023-06-12

### msccl

算法似乎可以混用, 似乎每处用到ccl的地方都会编译以下然后成功就使用?? be like: hier + allpair better than allpair only

- pipeline-model-parallel: 
    * hier got about 8% speedup
- tensor-model-parallel
    * speeddown
- data-parallel
    * speeddown


### On Optimizing the Communication of Model Parallelism

> overlap + loadbalance

- **Figure 3.**, 策略分析: T(A, B)表示从一个设备发到A个设备(每个设备B张卡)的时间
    * 一个一个发送: T(A, B) = ABt
    * 分B part, 分别发送到节点中的设备, 然后节点内部all gather
    * Send/recv + global all-gather
    * Broadcast
    * 本质: inter传部分, 再高速intra exchange, 再将这两个阶段overlap
- 负载均衡TODO:??
    * baseline: 全局指定一个固定传输顺序
    * load balance only: 贪心, 对任务的执行时间排序后，将任务分发给对应host
    * DFS剪枝: 全局给个全任务的排序, 然后任务Si的开始时间设置为 
    * TODO
- overlap key
    * observation: 计算i, i传输i+1, 计算i+1, 相互依赖不可overlap
        + 怎么个不可overlap法? 
            + 传输i+1的时候无所事事, e.g. Fig4: (a) mesh1要算fwd4, 但要先等fwd4接收完成
    * eager-f1b1:
        + 传输的时候计算其他batch的任务
        + 但相当于同时保存多个任务的数据, 内存开销增大
    * overlap backwrad: bw拆分成激活函数的梯度计算和参数的梯度计算, 两者没有依赖关系, 可以在这两个阶段之间插入任务


### megatron

- 模型并行
    * self-attention层
        + QKV拆分并行
        + 2 ar in fw + 2 ar in bw
    * MLP层
        + 矩阵行分(拆分数据), 不用sync(ccl)直接传递给dropout
        + 总体fw需要一次allreduce, bw需要一次allreduce


#### source code


##### DDP

- pytorch内部的DDP什么时候allreduce?
    * 数据并行的reduce在于(参考PS):
        + 需要所有数据来算梯度
        + 梯度算完后需要更新参数
    * **[Internal Design](https://pytorch.org/docs/stable/notes/ddp.html#internal-design)**
        1. construct: `state_dict`广播模型, 各自创建本地Reducer用于后面梯度同步
            - 以bucket为单位reduce
        2. fw
            - 数据输入本地模型
            - 自动求导分析需要模型的哪些参数来算bw, unused parameters mark as ready for reduce, bw只会等待unused parameters就reduce
            - reduce还是全部reduce, 只是稍微异步一点
        3. bw
            - 直接调用loss, 不受DDP控制, DDP只是在construct阶段插入一个sync的hook
            - hook将梯度数据mark as ready for reduce, 一个bucket满后就出发allreduce
            - 最后平均梯度写入`param.grad`
        4. 优化器(应用梯度)
            - 使用平均梯度优化本地模型即可
    * [DDP Internal](https://yi-wang-2005.medium.com/pytorch-distributeddataparallel-internals-c01c30a41192)
        1. Rank 0 broadcasts its randomly initialized local model to the other ranks
        2. DDP adjusts the behavior of l.backward() to compute average parameter gradients by registering hooks in PyTorch's autograd engine.
        3. DDP registers autograd post-hooks to nodes that compute parameter gradients. These post-hooks average the local gradients of each parameter across ranks

source code:

- ProcessGroup.hpp
    * ProcessGroup::broadcast() to send model states
    * ProcessGroup::allreduce() to sum gradients
- Store.hpp: to find each other.
- distributed.py: python端的DDP入口
    * `_sync_param` function performs intra-process parameter synchronization and broadcasts model
    * `Reducer.cpp` inter-process parameter synchronization(TODO 这样的话msccl的hier优化还有用么??)
- comm.h: 初始化阶段同步模型
- reducer.h: bw梯度同步的核心
    * Reducer: distributed.py中构造, 注册`Reducer::autograd_hook()` to gradient accumulators.
    * `autograd_hook()` function will be invoked by the autograd engine when a gradient becomes ready.
    * distributed.py中的`prepare_for_backward()`在fw结束后调用
        + It traverses the autograd graph to find unused parameters when `find_unused_parameters` is set to True in DDP constructor

```python
pretrain():
    # 准备data_iterator, TODO: 数据并行??
    build_train_valid_test_data_iterators()

train_step():
    # forward backward:
    # forward_pass -> forward_backward_func.
    forward_backward_func()
        forward_backward_pipelining_with_interleaving()
    # Reduce gradients.
    optimizer.reduce_model_grads(args, timers)
    # # Update parameters.
    update_successful, grad_norm, num_zeros_in_grad = optimizer.step(args, timers)
    # Gather params.
    if update_successful:
        optimizer.gather_model_params(args, timers)
```


bw hook, 添加allreduce hook, TODO, 但在哪触发呢? 直接`add_`??

```python
    def _make_param_hook(self, param):
        """Create the all-reduce hook for backprop."""
        # Hook used for back-prop.
        def param_hook(*unused):
            # Add the gradient to the buffer.
            if param.grad is not None:
                # The gradient function of linear layers is fused with GEMMs
                param.main_grad.add_(param.grad.data)
                # Now we can deallocate grad memory.
                param.grad = None
        return param_hook
```



- TODO: 
    * 看有没有创建communicator

- tricks!
    * training.py:393 We only support local DDP with multiple micro-batches.
        + local DDP: megatron自己实现的DDP, 用于处理multi micro batch的情况??
            + local DDP和torch DDP的区别









